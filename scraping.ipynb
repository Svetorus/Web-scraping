{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraping.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3BSiil5kcuYY3/9Z/T1I8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Svetorus/Web-scraping/blob/master/scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основы парсинга и работы с API"
      ],
      "metadata": {
        "id": "f3UE_sTHZbjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests"
      ],
      "metadata": {
        "id": "vlBdqsd3ZjBD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# метод get\n",
        "res = requests.get('https://netology.ru/blog/')\n",
        "# res\n",
        "res.status_code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZkDY_IPZmW7",
        "outputId": "7c121d03-c3ea-4bba-d33b-f72c02a5adfb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# браузер отрисовал бы страницу на основе данного текста\n",
        "res.text"
      ],
      "metadata": {
        "id": "S1FjogD3ZoAz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# получаем плохой статус\n",
        "bad_request = requests.get('https://netology.ru/blog/some_page')\n",
        "bad_request"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVeB8HNrZpyD",
        "outputId": "8eadd40c-c92b-4720-cb2e-a6808368bcfc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [404]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_request.text"
      ],
      "metadata": {
        "id": "y2ImtfRuZseT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# как разбирать всю эту разметку? Поможет BeautifulSoup.\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "-4QZOOm7b4F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создаем объекта, через методы которого будем искать нужные теги и извлекать их содержимое\n",
        "soup = BeautifulSoup(res.text)\n",
        "soup"
      ],
      "metadata": {
        "id": "B-t5ARITZzGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция finda_all позволяет найти все указанные тег с нужными атрибутами (с вложениями), возвращает список\n",
        "news = soup.find_all('div', class_='posts__item')\n",
        "print(len(news))\n",
        "print(news[0])"
      ],
      "metadata": {
        "id": "G4jGjdlDZzJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for el in news:\n",
        "    title = el.find('a', 'posts__link').text\n",
        "    print(title)\n",
        "    link = el.find('a', 'posts__link').get('href')\n",
        "    print(link)\n",
        "    date = el.find('div', 'posts__date').text\n",
        "    print(date)\n",
        "    category = el.find('a', 'tags__item').text\n",
        "    print(category)\n",
        "    print()"
      ],
      "metadata": {
        "id": "6eMqfAFHZzMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем функцию на основе всех предыдущих действий, которая будет возвращать датафрейм с датой, заголовком, категорией и ссылкой на полный текст поста"
      ],
      "metadata": {
        "id": "3OqjguJLZ-4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_netology_blog_posts():\n",
        "    url = 'https://netology.ru/blog/'\n",
        "    req = requests.get(url).text\n",
        "    soup = BeautifulSoup(req)\n",
        "\n",
        "    news = soup.find_all('div', class_='posts__item')\n",
        "\n",
        "    netology_blog = pd.DataFrame()\n",
        "\n",
        "    for el in news:\n",
        "        title = el.find('a', 'posts__link').text\n",
        "        link = el.find('a', 'posts__link').get('href')\n",
        "        date = el.find('div', 'posts__date').text\n",
        "        category = el.find('a', 'tags__item').text\n",
        "        row = {'date': date, 'title': title, 'link': link, 'category':category}\n",
        "        netology_blog = pd.concat([netology_blog, pd.DataFrame([row])]) \n",
        "    return netology_blog.reset_index(drop=True)\n",
        "\n",
        "get_netology_blog_posts()"
      ],
      "metadata": {
        "id": "c7OpYrlxZzPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А теперь поработаем поиском по сайту"
      ],
      "metadata": {
        "id": "QTndvS0_aFAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cформируем поисковый запрос, обратите внимание на его формат\n",
        "URL = 'https://netology.ru/blog/?s=python'"
      ],
      "metadata": {
        "id": "3UAvzk3HZzR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "req = requests.get(URL)\n",
        "req.text"
      ],
      "metadata": {
        "id": "jdLm-xypaCnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В функцию get можно передавать параметры и заголовки запроса в виде словаря"
      ],
      "metadata": {
        "id": "-uVZCAUYaN0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# в request можно передать параметры запроса и заголовки (headers) в виде словарей. \n",
        "URL = 'https://netology.ru/blog/'\n",
        "params = {\n",
        "    's': 'python'\n",
        "}\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Mobile Safari/537.36'\n",
        "}\n",
        "\n",
        "req = requests.get(URL, params=params, headers=headers)\n",
        "\n",
        "req.text"
      ],
      "metadata": {
        "id": "V1XWilYcaCqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(req.text)\n",
        "articles = soup.find_all('article', class_='status-publish')\n",
        "len(articles)"
      ],
      "metadata": {
        "id": "d_6YBL4SaCtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netology_blog = pd.DataFrame()\n",
        "for article in articles:\n",
        "    title = article.find('h2', class_='entry-title').text\n",
        "    link = article.find('h2', class_='entry-title').find('a').get('href')\n",
        "    date = article.find('span', class_='posted-on').text.strip()\n",
        "    category = article.find('div', class_='entry-cats').text\n",
        "    row = {'date': date, 'title': title, 'link': link, 'category':category}\n",
        "    netology_blog = pd.concat([netology_blog, pd.DataFrame([row])]) \n",
        "netology_blog"
      ],
      "metadata": {
        "id": "EfeF0TXpaCwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netology_blog = pd.DataFrame()\n",
        "for article in articles:\n",
        "    title = article.find('h2', class_='entry-title').text\n",
        "    link = article.find('h2', class_='entry-title').find('a').get('href')\n",
        "    date = article.find('span', class_='posted-on').text.strip()\n",
        "    if article.find('div', class_='entry-cats'):\n",
        "        category = article.find('div', class_='entry-cats').text\n",
        "    else:\n",
        "        category = 'Новость'\n",
        "    row = {'date': date, 'title': title, 'link': link, 'category':category}\n",
        "    netology_blog = pd.concat([netology_blog, pd.DataFrame([row])]) \n",
        "netology_blog"
      ],
      "metadata": {
        "id": "Uaxm-ddMaCzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А что если хотим собрать не только начальную выдачу, но и другие страницы? Изучам запросы при соответствующих действиях на сайте"
      ],
      "metadata": {
        "id": "ZJ-utq3JaX3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# адрес для отправки запроса\n",
        "url = 'https://netology.ru/blog/?infinity=scrolling'"
      ],
      "metadata": {
        "id": "U1Y3WTJPZzVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# параметры для отправки запроса\n",
        "params = {\n",
        "    'page': 1,\n",
        "    'query_args[s]': 'python'\n",
        "}"
      ],
      "metadata": {
        "id": "sgOokDR4aZMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# здесь мы используем post запрос\n",
        "res = requests.post(url, params=params)\n",
        "res.text"
      ],
      "metadata": {
        "id": "s8tsJiEraZQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(res.json()['html']) # извлекаем из ответа html-разметку\n",
        "display(soup)  "
      ],
      "metadata": {
        "id": "wKDshKbYaZTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles = soup.find_all('article', class_='status-publish')\n",
        "\n",
        "netology_blog = pd.DataFrame()\n",
        "for article in articles:\n",
        "    title = article.find('h2', class_='entry-title').text\n",
        "    link = article.find('h2', class_='entry-title').find('a').get('href')\n",
        "    date = article.find('span', class_='posted-on').text.strip()\n",
        "    if article.find('div', class_='entry-cats'):\n",
        "        category = article.find('div', class_='entry-cats').text\n",
        "    else:\n",
        "        category = 'Новость'    \n",
        "    row = {'date': date, 'title': title, 'link': link, 'category':category}\n",
        "    netology_blog = pd.concat([netology_blog, pd.DataFrame([row])]) \n",
        "netology_blog"
      ],
      "metadata": {
        "id": "T8LKncU9aZV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# напишем функцию по предыдущему коду\n",
        "def get_posts_from_blog():\n",
        "    url = 'https://netology.ru/blog/?infinity=scrolling'    \n",
        "    params = {\n",
        "        'page': 1,\n",
        "        'query_args[s]': 'python'\n",
        "    }\n",
        "    res = requests.post(url, params=params)\n",
        "\n",
        "    soup = BeautifulSoup(res.json()['html']) \n",
        "    articles = soup.find_all('article', class_='status-publish')\n",
        "\n",
        "    netology_blog = pd.DataFrame()\n",
        "    for article in articles:\n",
        "        title = article.find('h2', class_='entry-title').text\n",
        "        link = article.find('h2', class_='entry-title').find('a').get('href')\n",
        "        date = article.find('span', class_='posted-on').text.strip()\n",
        "        if article.find('div', class_='entry-cats'):\n",
        "            category = article.find('div', class_='entry-cats').text\n",
        "        else:\n",
        "            category = 'Новость'        \n",
        "        row = {'date': date, 'title': title, 'link': link, 'category':category}\n",
        "        netology_blog = pd.concat([netology_blog, pd.DataFrame([row])]) \n",
        "    return netology_blog.reset_index(drop=True)\n",
        "\n",
        "get_posts_from_blog()"
      ],
      "metadata": {
        "id": "WzgQoxaSaZYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А как извлечь посты с нескольких страниц?"
      ],
      "metadata": {
        "id": "EVnqNPPwaj-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# импортируем библиотеку для задержки исполнения кода\n",
        "import time\n",
        "\n",
        "def get_posts_from_blog(pages):\n",
        "    url = 'https://netology.ru/blog/?infinity=scrolling'    \n",
        "    params = {\n",
        "        'query_args[s]': 'python'\n",
        "    }\n",
        "\n",
        "    netology_blog = pd.DataFrame() \n",
        "    # добавим цикл с перебором страниц\n",
        "    for page in range(1, pages+1):\n",
        "        params['page'] = page\n",
        "        res = requests.post(url, params=params)\n",
        "        time.sleep(0.33)\n",
        "        soup = BeautifulSoup(res.json()['html']) \n",
        "        articles = soup.find_all('article', class_='status-publish')\n",
        "\n",
        "        for article in articles:\n",
        "            title = article.find('h2', class_='entry-title').text\n",
        "            link = article.find('h2', class_='entry-title').find('a').get('href')\n",
        "            date = article.find('span', class_='posted-on').text.strip()\n",
        "            if article.find('div', class_='entry-cats'):\n",
        "                category = article.find('div', class_='entry-cats').text\n",
        "            else:\n",
        "                category = 'Новость'     \n",
        "            row = {'date': date, 'title': title, 'link': link, 'category':category}\n",
        "            netology_blog = pd.concat([netology_blog, pd.DataFrame([row])]) \n",
        "\n",
        "    return netology_blog.reset_index(drop=True)\n",
        "\n",
        "get_posts_from_blog(5)"
      ],
      "metadata": {
        "id": "_KwOYQiSaZbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А если хотим добавить в датафрейм полные тексты всех постов?"
      ],
      "metadata": {
        "id": "jbDzfXalaq4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blog_posts = get_posts_from_blog(5)\n",
        "\n",
        "def add_full_text(posts_df):\n",
        "    i = 0\n",
        "    for el in posts_df['link']:\n",
        "        # print(el)\n",
        "        req = requests.get(el).text\n",
        "        time.sleep(0.33)\n",
        "        soup = BeautifulSoup(req)\n",
        "        full_text = soup.find('div', class_='entry-content').text.strip()\n",
        "        posts_df.loc[i, 'text'] = full_text\n",
        "        i += 1\n",
        "    return posts_df\n",
        "\n",
        "add_full_text(blog_posts)"
      ],
      "metadata": {
        "id": "-ezRjVrSaooo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем скрипт, который будет собирать новости с сайта Коммерсанта"
      ],
      "metadata": {
        "id": "EfMlkFGravhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://www.kommersant.ru/search/results'\n",
        "params = {\n",
        "    'search_query': 'python'\n",
        "}"
      ],
      "metadata": {
        "id": "Hu5fETl3aor4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get(URL, params)"
      ],
      "metadata": {
        "id": "VRwIG2U2aouy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.text"
      ],
      "metadata": {
        "id": "f9vYr5oDaoxi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(res.text)\n",
        "soup"
      ],
      "metadata": {
        "id": "saCAgCkbao0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# собираем все теги со ссылками на полные тексты новостей\n",
        "refs = soup.find_all('a', class_='uho__link')\n",
        "print(len(refs))\n",
        "print(refs)"
      ],
      "metadata": {
        "id": "FET3IX3EZzYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# добираемся до ссылок\n",
        "all_links = []\n",
        "\n",
        "for ref in refs:\n",
        "    all_links.append(ref.get('href'))\n",
        "\n",
        "print(len(all_links))\n",
        "print(all_links)"
      ],
      "metadata": {
        "id": "B_S956M3ZzbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# исключаем дубли\n",
        "all_links = set(all_links)\n",
        "print(len(all_links))"
      ],
      "metadata": {
        "id": "jigIC--5a-Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# формируем полноценные ссылки\n",
        "all_full_links = list(map(lambda x: 'https://www.kommersant.ru' + x, all_links))\n",
        "print(all_full_links)"
      ],
      "metadata": {
        "id": "A0hAadmEa-MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# объединим все в одну функцию\n",
        "def get_all_links(query):\n",
        "    url = 'https://www.kommersant.ru/search/results'\n",
        "    params = {\n",
        "        'search_query': query,\n",
        "    }\n",
        "    res = requests.get(URL, params)\n",
        "    soup = BeautifulSoup(res.text)\n",
        "    refs = soup.find_all('a', class_='uho__link')\n",
        "\n",
        "    all_links = []\n",
        "    for ref in refs:\n",
        "        all_links.append(ref.get('href'))\n",
        "    \n",
        "    all_links = set(all_links)\n",
        "\n",
        "    all_full_links = list(map(lambda x: 'https://www.kommersant.ru' + x, all_links))\n",
        "\n",
        "    return all_full_links\n",
        "\n",
        "all_links = get_all_links('python')\n",
        "print(all_links)"
      ],
      "metadata": {
        "id": "o7GQrYuOa-O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# но мы же собрали только одну страницу? Хотим ВСЕ новости\n",
        "def get_all_links(query, pages):\n",
        "    url = 'https://www.kommersant.ru/search/results'\n",
        "    links_list = []\n",
        "    params = {\n",
        "        'search_query': query\n",
        "    }\n",
        "    for i in range(1, pages+1):\n",
        "        params['page'] = i\n",
        "        res = requests.get(URL, params)\n",
        "        time.sleep(0.33)\n",
        "\n",
        "        refs = soup.find_all('a', class_='uho__link')\n",
        "        all_links = []\n",
        "\n",
        "        for ref in refs:\n",
        "            all_links.append(ref.get('href'))\n",
        "        \n",
        "        all_links = set(all_links)\n",
        "\n",
        "        all_full_links = list(map(lambda x: 'https://www.kommersant.ru' + x, all_links))\n",
        "\n",
        "        links_list += all_full_links\n",
        "    return links_list\n",
        "\n",
        "all_links = get_all_links('python', 3)\n",
        "print(len(all_links))\n",
        "print(all_links)"
      ],
      "metadata": {
        "id": "r-x6Eab4a-R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in all_links:\n",
        "    soup = BeautifulSoup(requests.get(link).text)\n",
        "    time.sleep(0.33)\n",
        "    date = pd.to_datetime(soup.find('time', class_='doc_header__publish_time').get('datetime'))\n",
        "    print(date)\n",
        "    title = soup.find('h1', class_='doc_header__name').text.strip()\n",
        "    print(title)\n",
        "    text = soup.find('div', class_='doc__body').text.strip()\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "Vd8CijaRa-Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# запишем данные в датафрейм\n",
        "kom_news = pd.DataFrame()\n",
        "for link in all_links:\n",
        "    soup = BeautifulSoup(requests.get(link).text)\n",
        "    time.sleep(0.3)\n",
        "    date = pd.to_datetime(soup.find('time', class_='doc_header__publish_time').get('datetime'))\n",
        "    title = soup.find('h1', class_='doc_header__name').text.strip()\n",
        "    text = soup.find('div', class_='doc__body').text.strip().replace('\\n', '')\n",
        "    row = {'date': date, 'title': title, 'text': text}\n",
        "    kom_news = pd.concat([kom_news, pd.DataFrame([row])])  \n",
        "kom_news"
      ],
      "metadata": {
        "id": "VY14b0l1a-Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# обернем в функцию \n",
        "def get_kom_news(links):\n",
        "    kom_news = pd.DataFrame()\n",
        "    for link in all_links:\n",
        "        soup = BeautifulSoup(requests.get(link).text)\n",
        "        time.sleep(0.3)\n",
        "        date = pd.to_datetime(soup.find('time', class_='doc_header__publish_time').get('datetime'))\n",
        "        title = soup.find('h1', class_='doc_header__name').text.strip()\n",
        "        text = soup.find('div', class_='doc__body').text.strip().replace('\\n', '')\n",
        "        row = {'date': date, 'title': title, 'text': text}\n",
        "        kom_news = pd.concat([kom_news, pd.DataFrame([row])])  \n",
        "    return kom_news.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "TWH6SAdQa-ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kom_news = get_kom_news(all_links)\n",
        "display(kom_news)"
      ],
      "metadata": {
        "id": "Sx31hAora-di"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}